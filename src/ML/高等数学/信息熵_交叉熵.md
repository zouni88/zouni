 事件的信息量随着事件发生概率的变大而 递减，信息不为负  
 两个不相关事件同时发生所产生的信息： h(x,y) = h(x) + h(y)  
 两个事件的概率满足： p(x,y) = p(x) * p(y).

对数形式的 真数相乘=>对数相加  

信息：
    𝐡(𝐱) = −𝒍𝒐𝒈𝟐𝒑(𝒙)

![信息熵](/res/math/entropy_1.png)  

熵：
    𝐇(𝐱) = −𝒔𝒖𝒎(𝒑(𝒙)𝒍𝒐𝒈𝟐𝒑(𝒙))

𝐟(𝐱) = −𝒍𝒐𝒈𝟐𝒙  函数图像
![-log2x](/res/math/entropy_2.png)

### 交叉熵

交叉熵： 两个事件的分布相似情况， H(p,q) = H(p) + KL(p,q)  
KL散度用来衡量真实分布和预测分布的差异情况  
假设 两个事件的概率分布相同则有：  
∵ p=q,则 KL(p,q)=0  
∴ H(p,q) = H(p)  

根据 以上推导可知：  
假设 p = [0,1,0]  
H(p) = -log2(p) = 0 # P事件的信息为0 惊喜度最低  
H(p,q) = 0 + KL(p,q) = KL(p,q)  
所以H(p,q) = -plog(q) = -1log(q) # 其实就是计算KL最小值 KL(p,q) = 0, p=q
